var documenterSearchIndex = {"docs":
[{"location":"mean-sd/#MeanSDMetric","page":"MeanSD","title":"MeanSDMetric","text":"MeanSDMetric is used for experimental endpoints reported as a mean together with a standard deviation (SD). It compares both the average level and the overall variability of individual-level simulation outputs to the corresponding experimental targets.\n\nSee API reference: MeanSDMetric.","category":"section"},{"location":"mean-sd/#Concept","page":"MeanSD","title":"Concept","text":"Many publications report a mean and SD but do not provide individual observations. MeanSDMetric treats these summary statistics as fixed targets and evaluates how well a simulated cohort matches them.\n\nThe loss is implemented as a sum of two terms:\n\na mean-matching term (same structure as MeanMetric),\nan SD-matching term that penalizes deviation of the simulated second central moment from the target variance.\n\nImplementation note. For consistency between mismatch and mismatch_expression, the simulated variance proxy is computed around the target mean rather than the simulated mean. This keeps the expression affine in binary selection variables (see below) and leads to a quadratic MIQP objective.","category":"section"},{"location":"mean-sd/#Example","page":"MeanSD","title":"Example","text":"Assume experimental data report a mean value of 2.5 and an SD of 0.6 based on a cohort of 50 patients:\n\nusing DigiPopData\n\nms_metric = MeanSDMetric(\n    50,     # experimental sample size\n    2.5,    # experimental mean\n    0.6     # experimental standard deviation\n)\n\nSimulated individual outcomes can be provided as a numeric vector:\n\nsim_values = randn(1000) .* 0.7 .+ 2.4\nloss = mismatch(sim_values, ms_metric)","category":"section"},{"location":"mean-sd/#Mathematics","page":"MeanSD","title":"Mathematics","text":"Let simulated individual values be y_1dotsy_N with sample mean\n\nmu_virt = frac1Nsum_i=1^N y_i\n\nLet the experimental targets be mu_exp and sigma (SD).","category":"section"},{"location":"mean-sd/#Mean-term","page":"MeanSD","title":"Mean term","text":"The mean mismatch term is:\n\nLambda_mu = Nfrac(mu_virt - mu_exp)^2sigma^2","category":"section"},{"location":"mean-sd/#SD-term-(variance-proxy)","page":"MeanSD","title":"SD term (variance proxy)","text":"In the current implementation, the simulated second central moment is evaluated around the target mean:\n\ns^2_virt = frac1Nsum_i=1^N(y_i - mu_exp)^2\n\nThe SD mismatch term penalizes deviation from the target variance sigma^2:\n\nLambda_sigma\n= fracN2frac(s^2_virt - sigma^2)^2sigma^4\n\nThe total loss is:\n\nLambda = Lambda_mu + Lambda_sigma\n\nThis form is likelihood-inspired (Gaussian second-order / chi-square style scaling) and is designed to be additive with other metric losses.","category":"section"},{"location":"mean-sd/#Quadratic-formulation-for-binary-selection","page":"MeanSD","title":"Quadratic formulation for binary selection","text":"For cohort selection, introduce a binary vector Xin01^N_tot where X_i=1 means that individual i is selected. Assume the selected cohort size is fixed: sum_i X_i = N_virt.\n\nThe selected-cohort mean is:\n\nmu_virt(X) = frac1N_virtsum_i=1^N_tot X_i y_i\n\nThe variance proxy used by the implementation is:\n\ns^2_virt(X) = frac1N_virtsum_i=1^N_tot X_i (y_i - mu_exp)^2\n\nBoth mu_virt(X) and s^2_virt(X) are affine in X for fixed N_virt. Therefore, each squared deviation term is a quadratic function of the binary decision variables, and the total loss Lambda(X) defines a Mixed-Integer Quadratic Programming (MIQP) objective.","category":"section"},{"location":"mean-sd/#Practical-notes","page":"MeanSD","title":"Practical notes","text":"size is stored for consistency and potential future extensions but is not used in the current loss.\nThe SD term uses a variance proxy computed around the target mean to preserve quadratic structure for MIQP.\nThe loss is intended for engineering workflows (calibration / selection) and can be summed with other metric losses.\nAs with all moment-based matching, rare heavy tails and strong outliers may affect SD matching and should be handled at the data preparation stage.","category":"section"},{"location":"quantile/#QuantileMetric","page":"Quantile","title":"QuantileMetric","text":"QuantileMetric is used for experimental endpoints reported as quantiles (e.g. median, quartiles, or arbitrary percentiles). It allows comparison of individual-level simulation results with aggregated quantile information published for a real population.\n\nThe metric does not attempt to reconstruct an underlying continuous distribution. Instead, quantile information is represented in a structured way that enables statistically consistent comparison with simulated data.\n\nSee API reference: QuantileMetric.","category":"section"},{"location":"quantile/#Concept","page":"Quantile","title":"Concept","text":"Experimental quantiles define thresholds that partition the outcome space into disjoint intervals. Each interval is associated with a known probability mass derived from the reported quantile levels.\n\nSimulated individual values are assigned to these intervals, and the resulting interval counts are compared to the expected probabilities. This reduces the quantile comparison problem to a categorical one, allowing reuse of the same statistical framework as in CategoryMetric.","category":"section"},{"location":"quantile/#Example","page":"Quantile","title":"Example","text":"Assume experimental data report the 25%, 50%, and 75% quantiles of a biomarker measured in a cohort of 80 patients:\n\n25% quantile: 1.2\n50% quantile (median): 1.8\n75% quantile: 2.6\n\nThis can be encoded as a QuantileMetric:\n\nusing DigiPopData\n\nq_metric = QuantileMetric(\n    80,                         # experimental sample size\n    [0.25, 0.50, 0.75],         # quantile levels\n    [1.2, 1.8, 2.6],            # quantile values\n    skip_nan = true             # ignore NaN values in simulations\n)\n\nHere, quantile levels must be strictly increasing and lie in the open interval (0, 1). Quantile values must also be sorted in ascending order.","category":"section"},{"location":"quantile/#Comparison-with-simulated-data","page":"Quantile","title":"Comparison with simulated data","text":"Assume a virtual population of simulated individuals, where each individual produces a single numeric outcome.\n\nSimulation results can be provided as a vector:\n\nsim_values = randn(1000) .+ 1.8\n\nNaN values in simulation data are allowed if skip_nan = true and are excluded from the comparison.\n\nEach simulated value is assigned to one of the intervals defined by the quantiles, and the resulting interval counts are used to compute the mismatch.\n\nTo compute the loss, we can use the mismatch function:\n\nloss = mismatch(sim_values, q_metric)","category":"section"},{"location":"quantile/#Mathematics","page":"Quantile","title":"Mathematics","text":"Let reported quantile levels be\n\n0  q_1  q_2  dots  q_m  1\n\nwith corresponding quantile values\n\nv_1  v_2  dots  v_m\n\nThese values define m+1 disjoint intervals:\n\n(-infty v_1)\nv_1 v_2)\ndots\nv_m +infty)\n\nThe associated experimental probabilities are:\n\np_1 = q_1quad\np_i = q_i - q_i-1 (i=2dotsm)quad\np_m+1 = 1 - q_m\n\nFor a virtual population of size N, simulated values produce interval counts\n\nveck = (k_1 dots k_m+1) quad sum_i k_i = N\n\nUnder this construction, interval counts follow a multinomial distribution. As in CategoryMetric, a Gaussian approximation of the multinomial likelihood is used, leading to the quadratic loss:\n\nLambda\napprox\n(veck - Nvecp)^T\nSigma^-1\n(veck - Nvecp)\n\nwhere\n\nSigma = N (mathrmdiag(vecp) - vecpvecp^T)\n\nOne interval is removed to eliminate linear dependence, and the reduced quadratic form is evaluated.","category":"section"},{"location":"quantile/#Practical-notes","page":"Quantile","title":"Practical notes","text":"Quantile-based comparison is robust to outliers and suitable for skewed or heavy-tailed distributions.\nNo assumptions are made about the shape of the underlying distribution beyond the reported quantiles.\nThe resulting loss is a Gaussian approximation of a likelihood and can be combined with other likelihood-based metrics.\nThe quadratic form enables use in optimization and mixed-integer quadratic programming workflows.","category":"section"},{"location":"survival/#SurvivalMetric","page":"Survival","title":"SurvivalMetric","text":"SurvivalMetric is used for experimental endpoints reported as survival curves or, more generally, as survival levels associated with ordered values. Typical examples include time-to-event data, progression-free survival, or any endpoint where the probability of remaining below or above a threshold is reported.\n\nThe metric does not assume a specific time scale or hazard model. Survival levels are treated as cumulative probabilities that define intervals over an ordered outcome axis.\n\nSee API reference: DigiPopData.SurvivalMetric.","category":"section"},{"location":"survival/#Concept","page":"Survival","title":"Concept","text":"Experimental survival data are typically reported as decreasing survival levels (e.g. 0.9, 0.8, 0.7) associated with increasing values (e.g. time points or threshold values).\n\nThese survival levels define a partition of the outcome space into disjoint intervals. Each interval corresponds to the fraction of the population whose outcome falls between two consecutive survival levels.\n\nSimulated individual outcomes are assigned to these intervals, and the resulting interval counts are compared with the expected probabilities. This reduces survival comparison to the same categorical framework used by CategoryMetric and QuantileMetric.","category":"section"},{"location":"survival/#Example","page":"Survival","title":"Example","text":"Assume experimental data report survival levels at increasing time points for a cohort of 120 patients:\n\nSurvival 0.9 at time 5\nSurvival 0.7 at time 10\nSurvival 0.4 at time 20\n\nThis can be encoded as a SurvivalMetric:\n\nusing DigiPopData\n\ns_metric = SurvivalMetric(\n    120,                # experimental sample size\n    [0.9, 0.7, 0.4],    # survival levels (descending)\n    [5.0, 10.0, 20.0]   # corresponding values (ascending)\n)\n\nSurvival levels must be sorted in descending order, while the associated values must be sorted in ascending order.","category":"section"},{"location":"survival/#Comparison-with-simulated-data","page":"Survival","title":"Comparison with simulated data","text":"Assume a virtual population where each individual produces a single event time (or an ordered outcome value).\n\nSimulation results are provided as a numeric vector:\n\nusing Random\nsim_times = randexp(1000)\n\nEach simulated value is assigned to one of the survival intervals defined by the experimental data. The resulting interval counts are used to compute the mismatch.\n\nTo compute the loss, we can use the mismatch function:\n\nloss = mismatch(sim_times, s_metric)","category":"section"},{"location":"survival/#Mathematics","page":"Survival","title":"Mathematics","text":"Let reported survival levels be\n\n1 ge s_1  s_2  dots  s_m ge 0\n\nwith corresponding ordered values\n\nv_1  v_2  dots  v_m\n\nThese values define m+1 disjoint intervals:\n\n(-infty v_1)\nv_1 v_2)\ndots\nv_m +infty)\n\nThe associated interval probabilities are derived as differences of successive survival levels:\n\np_1 = 1 - s_1quad\np_i = s_i-1 - s_i (i=2dotsm)quad\np_m+1 = s_m\n\nFor a virtual population of size N, simulated values produce counts\n\nveck = (k_1 dots k_m+1) quad sum_i k_i = N\n\nUnder this construction, interval counts follow a multinomial distribution. As in CategoryMetric and QuantileMetric, a Gaussian approximation of the multinomial likelihood is used, leading to the quadratic loss:\n\nLambda\napprox\n(veck - Nvecp)^T\nSigma^-1\n(veck - Nvecp)\n\nwhere\n\nSigma = N (mathrmdiag(vecp) - vecpvecp^T)\n\nOne interval is removed to eliminate linear dependence, and the reduced quadratic form is evaluated.","category":"section"},{"location":"survival/#Practical-notes","page":"Survival","title":"Practical notes","text":"Survival information is treated as cumulative probability data, without assuming a specific hazard or time model.\nThe resulting loss is a Gaussian approximation of a likelihood and can be combined with other likelihood-based metrics.\nThe quadratic form enables use in optimization and mixed-integer quadratic programming workflows.","category":"section"},{"location":"simulation-data-format/#Simulation-Data-Format","page":"Simulation Data Format","title":"Simulation Data Format","text":"Simulated data are provided to DigiPopData.jl as individual-level results of QSP model simulations. They are represented as a tabular structure and are typically produced by an external simulator.","category":"section"},{"location":"simulation-data-format/#Structure","page":"Simulation Data Format","title":"Structure","text":"Simulation data are expected to be loaded as a DataFrame with the following columns:\n\nid   Identifier of a virtual patient.   Used to distinguish individuals and for selection or weighting procedures.   Type: String, or Int64\nscenario   Identifier of a simulation scenario (e.g. treatment, dosing regimen, condition).   Allows multiple scenarios per virtual patient.   Type: String.\n<endpoint columns>   One or more columns containing simulated model outputs that correspond to experimental metrics (e.g. concentration, biomarker value, event time).   Column names must match the identifiers referenced by experimental metrics. Type: String, Int64, or Float64\n\nEach row corresponds to one simulated individual under one scenario. For a given workflow, it is typically expected that all required endpoint values are present.","category":"section"},{"location":"simulation-data-format/#CSV-file-format","page":"Simulation Data Format","title":"CSV file format","text":"Simulation results can be stored and loaded from a CSV file. This is particularly useful when simulation and analysis are performed in separate tools or when virtual patient selection is applied to precomputed results.\n\nSimulation data can be loaded as follows:\n\nusing CSV, DataFrames\n\nsimulation_df = CSV.File(\"simulation_data.csv\") |> DataFrame","category":"section"},{"location":"simulation-data-format/#Example","page":"Simulation Data Format","title":"Example","text":"Example of a simulation table for a virtual population:\n\nid scenario conc_t24 conc_t48 biomarker_t48 response\nVP1 placebo 1.23 0.45 13 NonResponder\nVP2 placebo 0.98 0.52 10 NonResponder\nVP3 placebo 1.10 0.48 12 NonResponder\nVP1 treated 3.42 0.30 8 Responder\nVP2 treated 2.95 0.33 4 NonResponder\n\nIn this example:\n\nid identifies virtual patients,\nscenario distinguishes treatment conditions,\nconc_t24, conc_t48, and biomarker_t48 are simulated numerical endpoints,\nresponse is a categorical simulated endpoint, that can be referenced by experimental metrics.\n\nThis table can be directly used together with experimental metric definitions to compute mismatch and loss values.","category":"section"},{"location":"getting-started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This document provides a brief overview of how to get started with the DigiPopData.jl package. It covers installation, basic usage, and examples of implemented metrics for comparing simulated populations with experimental data.\n\nCalculation of the loss function for the SurvivalMetric:\n\nusing DigiPopData\n\n# create metric based on Survival data\nmetric1 = SurvivalMetric(\n    150,    # number of patients in the real population\n    [0.8111, 0.3480, 0.2852, 0.2538, 0.2307, 0.2307, 0.1818, 0.1338], # survival values in descending order\n    [2., 5., 8., 10., 12., 15., 20., 25.] # time points\n)\n\n# calculate loss function for data on metric -2ln(Likelihood)\nloss_value = mismatch(\n    [2., 1.4, 4.4, 6., 7.89], # individual survival times for 5 patients\n    metric1\n)\n\nCompute the total loss between real and virtual populations defined in CSV files:\n\nusing DigiPopData\nusing DataFrames, CSV\n\n# Load the real population data from CSV file\nmetrics_df = CSV.read(\"metrics.csv\", DataFrame)\nmetrics = parse_metric_bindings(metrics_df)\n\n# Load the virtual population data from CSV file\nvirtual_df = CSV.read(\"virtual_population.csv\", DataFrame)\n\nloss = get_loss(virtual_df, metrics)","category":"section"},{"location":"mean/#MeanMetric","page":"Mean","title":"MeanMetric","text":"MeanMetric is used for experimental endpoints reported as a mean value with an associated variability measure. It compares the mean of individual-level simulation outputs to a target experimental mean.\n\nThe metric assumes that experimental summary statistics are fixed targets and does not attempt to model uncertainty of the experimental mean itself.\n\nSee API reference: MeanMetric.","category":"section"},{"location":"mean/#Concept","page":"Mean","title":"Concept","text":"Many experimental datasets report only a population mean and a standard deviation, without providing individual-level observations. In such cases, model calibration often proceeds by matching simulated population averages to reported mean values.\n\nMeanMetric formalizes this comparison by measuring the discrepancy between the mean of simulated individuals and the experimental target mean, scaled by the reported variability.\n\nThe experimental standard deviation is treated as a known scale parameter and is used to normalize the mismatch.","category":"section"},{"location":"mean/#Example","page":"Mean","title":"Example","text":"Assume experimental data report a mean biomarker value of 2.5 with a standard deviation of 0.6, based on a cohort of 50 patients.\n\nThis can be encoded as a MeanMetric:\n\nusing DigiPopData\n\nm_metric = MeanMetric(\n    50,     # experimental sample size\n    2.5,    # experimental mean\n    0.6     # experimental standard deviation\n)\n\nAssume a virtual population where each individual produces a single numeric outcome:\n\nsim_values = randn(1000) .* 0.6 .+ 2.5\n\nThe mismatch between simulation and experiment is computed as:\n\nloss = mismatch(sim_values, m_metric)","category":"section"},{"location":"mean/#Mathematics","page":"Mean","title":"Mathematics","text":"Let simulated individual values be y_1 dots y_N, with sample mean\n\nmu_virt = frac1N sum_i=1^N y_i\n\nLet the experimental target be a mean value mu_exp with reported standard deviation sigma.\n\nThe mismatch is computed as:\n\nLambda\n=\nN frac(mu_virt - mu_exp)^2sigma^2\n\nThis expression corresponds to a Gaussian (second-order) approximation of the negative log-likelihood under the assumption that individual observations are normally distributed with known variance sigma^2.","category":"section"},{"location":"mean/#Quadratic-formulation-for-binary-selection","page":"Mean","title":"Quadratic formulation for binary selection","text":"In virtual population selection problems, each simulated individual is either included or excluded from the selected cohort. This is represented by a binary selection vector X in 01^N_tot, where X_i = 1 indicates that individual i is selected.\n\nLet simulated individual outcomes be y_1 dots y_N_tot. The mean of the selected virtual cohort of fixed size N_virt is\n\nmu_virt(X) = frac1N_virt sum_i=1^N_tot X_i y_i\n\nThe mismatch defined by MeanMetric can then be written as\n\nLambda(X)\n=\nN_virt\nfrac(mu_virt(X) - mu_exp)^2sigma^2\n\nSubstituting the expression for mu_virt(X) yields\n\nLambda(X)\n=\nfrac1sigma^2 N_virt\nleft( sum_i=1^N_tot X_i y_i - N_virt mu_exp right)^2\n\nThis expression is a quadratic function of the binary variables X. Therefore, virtual population selection based on MeanMetric can be formulated as a Mixed-Integer Quadratic Programming (MIQP) problem, provided that the cohort size N_virt is fixed (e.g. enforced via a constraint sum_i X_i = N_virt).","category":"section"},{"location":"mean/#Practical-notes","page":"Mean","title":"Practical notes","text":"The reported standard deviation is treated as a fixed scale parameter and is not re-estimated from simulated data.\nThe experimental sample size is stored for consistency with other metrics but is not currently used in the loss computation.\nThe resulting loss is likelihood-based and can be combined with other metric losses.\nThe quadratic form enables use in optimization workflows, including mixed-integer quadratic programming formulations.","category":"section"},{"location":"category/#CategoryMetric","page":"Category","title":"CategoryMetric","text":"CategoryMetric is used for experimental endpoints where the outcome for each individual is a categorical label. Typical examples include responder status, disease subtype, toxicity grade, or any discrete classification without an inherent numeric scale.\n\nThis metric compares the distribution of categories observed in an experiment with the distribution produced by individual-level simulations.\n\nSee also: CategoryMetric.","category":"section"},{"location":"category/#Example","page":"Category","title":"Example","text":"Consider a clinical study evaluating a new treatment. Each patient is classified into one of three response categories:\n\nResponder\nNon-Responder\nPartial Responder\n\nThe experimental cohort consists of 100 patients with the following distribution:\n\nResponder: 40  \nNon-Responder: 50  \nPartial Responder: 10  \n\nThis information can be encoded as a CategoryMetric as follows:\n\nusing DigiPopData\n\ncat_metric1 = CategoryMetric(\n   100, # size\n   [\"Responder\", \"Non-Responder\", \"Partial Responder\"], # groups\n   [0.4, 0.5, 0.1] # rates\n)\n\nHere, the experimental data are represented as population-level proportions, without access to individual patient outcomes.\n\n##Comparison with simulated data\n\nSimulation results are stored as a vector of categorical outcomes, where each element corresponds to one virtual patient.\n\nresp = rand([\"Responder\", \"Non-Responder\", \"Partial Responder\"], 1000)\n\nTo compute the loss, we can use the mismatch function:\n\nloss = mismatch(resp, cat_metric1)\n\nThis will calculate the discrepancy between the observed category distribution in the experimental data and the distribution derived from the simulated individual outcomes.","category":"section"},{"location":"category/#Mathematics","page":"Category","title":"Mathematics","text":"CategoryMetric compares simulated categorical outcomes with experimental category frequencies. Let an experiment define a categorical distribution with probabilities vecp = (p_1 dots p_g) estimated from an experimental cohort of size hatN.\n\nA virtual population of size N produces counts veck = (k_1 dots k_g), where sum_i k_i = N.","category":"section"},{"location":"category/#Exact-likelihood","page":"Category","title":"Exact likelihood","text":"In principle, simulated counts follow a multinomial distribution:\n\n(k_1 dots k_g) sim mathrmMultinomial(N vecp)\n\nThe corresponding negative log-likelihood is:\n\n-2 log L\n= -2 sum_i=1^g k_i log p_i\n+ 2 log(N)\n- 2 sum_i=1^g log(k_i)\n\nThis exact formulation is rarely used directly in practice due to numerical instability and strong parameter coupling.","category":"section"},{"location":"category/#Gaussian-approximation","page":"Category","title":"Gaussian approximation","text":"For sufficiently large N and non-degenerate probabilities, the multinomial distribution can be approximated by a multivariate normal distribution:\n\nveck sim mathcalN(Nvecp Sigma)\n\nwith covariance matrix:\n\nSigma = N left( mathrmdiag(vecp) - vecpvecp^T right)\n\nThis covariance matrix is singular due to the constraint sum_i k_i = N. To obtain a non-singular representation, one category is removed, and only the first g-1 categories are used.\n\nThe resulting mismatch (loss) is computed as a Mahalanobis distance:\n\nLambda\napprox\n(veck - Nvecp)^T Sigma^-1 (veck - Nvecp)\n\nLower values of Lambda indicate better agreement between simulated and experimental category distributions.","category":"section"},{"location":"category/#Quadratic-formulation-for-binary-selection","page":"Category","title":"Quadratic formulation for binary selection","text":"In virtual population selection problems, each simulated individual is either included or excluded from the cohort. This is represented by a binary selection vector X in 01^N_tot, where X_i = 1 indicates that virtual patient i is selected.\n\nFor a categorical endpoint with g groups, define indicator vectors Z_1 dots Z_g-1 in 01^N_tot, where (Z_j)_i = 1 if patient i belongs to category j. The number of selected patients in each category is then given by k_j = Z_j^T X.\n\nUsing the Gaussian approximation of the multinomial distribution, the mismatch between simulated and experimental category distributions can be written as a Mahalanobis distance:\n\nLambda(X)\n=\n(veck - N_virt vecp)^T\nSigma^-1\n(veck - N_virt vecp)\n\nwhere vecp are experimental category probabilities, N_virt = mathbf1^T X, and Sigma = N_virt(mathrmdiag(vecp) - vecpvecp^T).\n\nSubstituting veck = Z^T X shows that Lambda(X) is a quadratic form with respect to the binary decision variables X. As a result, cohort selection based on CategoryMetric can be formulated as a Mixed-Integer Quadratic Programming (MIQP) problem.","category":"section"},{"location":"category/#Practical-notes","page":"Category","title":"Practical notes","text":"CategoryMetric operates on aggregated categorical frequencies and does not require access to individual-level experimental data.\nCategories with zero experimental probability are automatically marked as inactive and excluded from the loss computation. This avoids numerical issues while preserving the correct degrees of freedom.\nThe loss is computed as a Gaussian (second-order) approximation of the multinomial negative log-likelihood. As a result, it can be interpreted as a likelihood-based discrepancy and safely combined with other likelihood-based metrics.\nThe metric is insensitive to the ordering of category labels. Only category membership and experimental probabilities are used.\nReliable results require sufficiently large virtual populations and non-degenerate experimental probabilities. Extremely rare categories may require special consideration or aggregation at the data preparation stage.","category":"section"},{"location":"api/#DigiPopData.jl-API","page":"API","title":"DigiPopData.jl API","text":"","category":"section"},{"location":"api/#DigiPopData.AbstractMetric","page":"API","title":"DigiPopData.AbstractMetric","text":"AbstractMetric\n\nAbstract super‑type for all metric descriptors used by DigiPopData.\n\nPurpose\n\nGroup together heterogeneous metrics (Mean, MeanSD, Category, …) so they can share the same dispatch points (mismatch, mismatch_expression, get_loss, ...).\n\nRequired interface\n\nmismatch: Function to calculate the loss for a given metric and simulated data as a value.\nmismatch_expression: Function to calculate the loss for a given metric and simulated data as an expression.\nvalidate: Function to validate the simulated data against the metric.\n\nThe parsing rules for the metric type are defined in the PARSERS dictionary to convert from DataFrame row to specific Metric struture. It is used in the parse_metric_bindings method.\n\nPARSERS[\"<metric_type>\"] = (row) -> begin\n    # parsing logic\nend\n\n\n\n\n\n","category":"type"},{"location":"api/#DigiPopData.CategoryMetric","page":"API","title":"DigiPopData.CategoryMetric","text":"CategoryMetric <: AbstractMetric\n\nCategoryMetric is a metric descriptor for categorical data. It is based on polinomial distribution within the groups. \n\nFields\n\nsize::Int: The size of the dataset.\ngroups::Vector{String}: The names of the groups.\nrates::Vector{Float64}: The probabilities of each group.\ncov_inv::Matrix{Float64}: The inverse of the covariance matrix of the groups.\ngroup_active::Vector{Bool}: A boolean vector indicating which groups are active (non-zero rates).\n\nConstructor\n\nCategoryMetric(size::Int, groups::Vector{String}, rates::Vector{Float64}): Creates a new instance of CategoryMetric.  It validates the input data and calculates the inverse covariance matrix.\n\n\n\n\n\n","category":"type"},{"location":"api/#DigiPopData.MeanMetric","page":"API","title":"DigiPopData.MeanMetric","text":"MeanMetric <: AbstractMetric\n\nA metric that compares the mean of a simulated dataset to a target mean.\n\nFields\n\nsize::Int: The size of the dataset.\nmean::Float64: The target mean value.\nsd::Float64: The target standard deviation value.\n\n\n\n\n\n","category":"type"},{"location":"api/#DigiPopData.MeanSDMetric","page":"API","title":"DigiPopData.MeanSDMetric","text":"MeanSDMetric <: AbstractMetric\n\nA metric that compares the mean and standard deviation (SD) of a simulated dataset to a target mean and SD.\n\nFields\n\nsize::Int: The size of the dataset.\nmean::Float64: The target mean value.\nsd::Float64: The target standard deviation value.\n\n\n\n\n\n","category":"type"},{"location":"api/#DigiPopData.MetricBinding","page":"API","title":"DigiPopData.MetricBinding","text":"MetricBinding is container that binds a scenario, an endpoint and a concrete AbstractMetric description into a single unit that can be logged, displayed or passed to optimisation / validation routines.\n\nFields\n\nName Type Description\nid String Unique identifier of the binding\nscenario String Scenario (e.g. simulation arm) in which the metric is evaluated\nmetric AbstractMetric Metric implementation (MeanMetric, CategoryMetric, …)\nendpoint String Observable / model variable the metric is computed for\nactive Bool Whether the binding is enabled (true by default)\n\n\n\n\n\n","category":"type"},{"location":"api/#DigiPopData.QuantileMetric","page":"API","title":"DigiPopData.QuantileMetric","text":"QuantileMetric <: AbstractMetric\n\nQuantileMetric is a metric descriptor for quantile data. It is based on the quantiles of the data and their corresponding values.\n\nFields\n\nsize::Int: The size of the dataset.\nlevels::Vector{Float64}: The quantile levels (e.g. 0.25, 0.5, 0.75).\nvalues::Vector{Float64}: The corresponding values for the quantile levels.\nskip_nan::Bool: If true, NaN values are allowed in simulated data and will be ignored. Iffalse`, NaN values are not allowed.\ncov_inv::Matrix{Float64}: The inverse of the covariance matrix of the groups.\ngroup_active::Vector{Bool}: A boolean vector indicating which groups are active (non-zero rates).\nrates::Vector{Float64}: The probabilities of each group.\n\nConstructor\n\nQuantileMetric(size::Int, levels::Vector{Float64}, values::Vector{Float64}; skip_nan::Bool = false): Creates a new instance of QuantileMetric.  It validates the input data and calculates the inverse covariance matrix.\n\n\n\n\n\n","category":"type"},{"location":"api/#DigiPopData.SurvivalMetric","page":"API","title":"DigiPopData.SurvivalMetric","text":"\"     SurvivalMetric <: AbstractMetric\n\nFields\n\nsize::Int: The size of the dataset.\nlevels::Vector{Float64}: The survival levels (e.g. 0.9, 0.8, 0.7).\nvalues::Vector{Float64}: The corresponding values for the survival levels.\ncov_inv::Matrix{Float64}: The inverse of the covariance matrix of the groups.\ngroup_active::Vector{Bool}: A boolean vector indicating which groups are active (non-zero rates).\nrates::Vector{Float64}: The probabilities of each group.\n\nConstructor\n\nSurvivalMetric(size::Int, levels::Vector{Float64}, values::Vector{Float64}): Creates a new instance of SurvivalMetric.  It validates the input data and calculates the inverse covariance matrix.\n\n\n\n\n\n","category":"type"},{"location":"api/#DigiPopData.get_loss-Tuple{DataFrames.DataFrame, Vector{MetricBinding}}","page":"API","title":"DigiPopData.get_loss","text":"get_loss(simulated::DataFrame, metric_bindings::Vector{MetricBinding}) -> Float64\n\nCalculate the loss for a given set of metric bindings and a simulated DataFrame. The function iterates over the metric bindings, selecting the relevant data from the simulated DataFrame  based on the scenario and endpoint specified in each binding. It then computes the loss using the mismatch  function defined in the metric.\n\nArguments\n\nsimulated::DataFrame: A DataFrame containing the simulated data.\nmetric_bindings::Vector{MetricBinding}: A vector of MetricBinding objects, each containing a scenario,\n\nendpoint, and metric.\n\n\n\n\n\n","category":"method"},{"location":"api/#DigiPopData.mismatch-Tuple{AbstractVector{<:Real}, DigiPopData.AbstractMetric}","page":"API","title":"DigiPopData.mismatch","text":"mismatch(sim::AbstractVector{<:Real}, metric::AbstractMetric) -> Float64\n\nArguments\n\nsim::AbstractVector{<:Real}: A vector of simulated data.\nmetric::AbstractMetric: An instance of a metric descriptor (e.g., MeanMetric, CategoryMetric, etc.).\n\nReturn a loss that quantifies the mismatch between simulated data sim and the target metric metric.   The concrete formula depends on the subtype of AbstractMetric.\n\n\n\n\n\n","category":"method"},{"location":"api/#DigiPopData.mismatch_expression-Tuple{AbstractVector{<:Real}, DigiPopData.AbstractMetric, Vector{JuMP.VariableRef}, Int64}","page":"API","title":"DigiPopData.mismatch_expression","text":"mismatch_expression(sim::AbstractVector{<:Real}, metric::AbstractMetric, X::Vector{VariableRef}, X_len::Int) -> QuadExpr\n\nArguments\n\nsim::AbstractVector{<:Real}: A vector of simulated data.\nmetric::AbstractMetric: An instance of a metric descriptor (e.g., MeanMetric, CategoryMetric, etc.).\nX::Vector{VariableRef}: A vector of JuMP variable references.\nX_len::Int: The length of the vector of JuMP variable references.\n\nReturn an expression that quantifies the mismatch between simulated data sim and the target metric metric. The concrete formula depends on the subtype of AbstractMetric.\n\n\n\n\n\n","category":"method"},{"location":"api/#DigiPopData.parse_metric_bindings-Tuple{DataFrames.DataFrame}","page":"API","title":"DigiPopData.parse_metric_bindings","text":"parse_metric_bindings(df::DataFrame) -> Vector{MetricBinding}\n\nParse a DataFrame with metric‑binding definitions and return a vector of MetricBinding objects.\n\nThe DataFrame should contain the following columns:\n\nid: Unique identifier for the metric binding.\nscenario: The scenario to which the metric binding applies.\nendpoint: The observable (endpoint) associated with the metric binding.\nactive: (optional) A boolean indicating whether the metric binding is active (default is true).\nmetric.type: The type of metric (e.g., \"mean\", \"category\", etc.).\nmetric.<parameter>: Additional parameters for the metric, depending on its type.\n\nThe function uses the PARSERS dictionary to find the appropriate parser for the metric type. The function iterates over each row of the DataFrame, extracting the relevant information and creating a MetricBinding object.\n\n\n\n\n\n","category":"method"},{"location":"api/#DigiPopData.validate-Tuple{AbstractVector{<:Real}, DigiPopData.AbstractMetric}","page":"API","title":"DigiPopData.validate","text":"validate(sim::AbstractVector{<:Real}, metric::AbstractMetric)\n\nArguments\n\nsim::AbstractVector{<:Real}: A vector of simulated data.\nmetric::AbstractMetric: An instance of a metric descriptor (e.g., MeanMetric, CategoryMetric, etc.).\n\nValidate the simulated data sim against the target metric metric. It throws an error if the validation fails. The concrete validation rules depend on the subtype of AbstractMetric.\n\n\n\n\n\n","category":"method"},{"location":"metrics/#Overview-of-Metrics","page":"Overview","title":"Overview of Metrics","text":"Each metric compares simulated individuals with experimental data based on the following statistics:\n\nJulia struct metric.type in DataFrame specific properties BIP support Description\nMeanMetric mean mean, sd + Compare the mean.\nMeanSDMetric mean_sd mean, sd + Compare the mean and standard deviation.\nCategoryMetric category groups, rates + Compare the categorical distribution.\nQuantileMetric quantile levels, values + Compare the quantile values.\nSurvivalMetric survival levels, values + Compare the survival curves.\n\nBIP support indicates whether the metric is supported for Binary Integer Programming optimization, for example in VPopMIP.jl package.","category":"section"},{"location":"key-concepts/#Key-Concepts","page":"Key Concepts","title":"Key Concepts","text":"","category":"section"},{"location":"key-concepts/#The-core-mismatch-problem","page":"Key Concepts","title":"The core mismatch problem","text":"In practice, experimental data reported in publications are usually available only as aggregated summary statistics (e.g. mean, median, quantiles, survival curves), while QSP models naturally produce individual-level simulations. DigiPopData.jl provides a structured way to bridge this mismatch.\n\nIn many modeling workflows, models are calibrated against mean values, effectively treating the system as deterministic. This approach ignores inter-individual variability and is often insufficient when modeling heterogeneous patient populations, where the distribution of outcomes is clinically and mechanistically relevant.\n\nWhen individual patient data are available, established approaches such as NLME modeling or distribution-based statistical tests (e.g. Kolmogorov–Smirnov) can be used to compare simulations and observations. However, in many realistic scenarios individual-level experimental data are not accessible, and only population-level summary statistics are reported.\n\nIn this setting, there is no standard representation of experimental targets, and no unified way to define a statistically meaningful mismatch between individual simulations and aggregated data. As a result, comparison rules and loss functions are often implemented in an ad hoc manner, making results difficult to reproduce, justify, and compare across studies.\n\nThis problem is particularly relevant in QSP modeling and virtual patient workflows, where individual simulations must be evaluated against published population-level endpoints.","category":"section"},{"location":"key-concepts/#Objectives-of-DigiPopData.jl","page":"Key Concepts","title":"Objectives of DigiPopData.jl","text":"DigiPopData.jl is designed to provide a consistent and reusable framework for comparing individual-level simulations with aggregated experimental data.\n\nThe main objectives of the package are:\n\nUnified representation of experimental summary data. Provide a general and extensible format for representing experimental data with variability at the population level. Experimental and simulation data are stored in structured formats and can be provided as tables (e.g. DataFrame or CSV), enabling reproducible and tool-independent workflows. Users may select existing metric types or define custom ones when needed.\nStatistically grounded loss functions. Define Objective Function Values (OFV) that compare individual-level simulation outputs with aggregated experimental statistics. These loss functions are designed to be statistically interpretable and suitable for use in calibration, model comparison, and virtual population workflows.\nReusable metric objects independent of downstream methods. Provide metric objects that encapsulate experimental targets and comparison rules, allowing them to be reused across different methodologies, such as parameter optimization, virtual population selection, or weighting approaches.\n\nCurrently, DigiPopData.jl focuses on population-level representations of experimental data. Support for individual-level experimental data may be added in the future to enable more complete use of available information.","category":"section"},{"location":"key-concepts/#Objects-and-data-flow","page":"Key Concepts","title":"Objects and data flow","text":"At a high level, DigiPopData.jl can be understood as a sequence of well-defined data transformation and evaluation steps.\n\nThe workflow starts with the collection of experimental data that reflect variability in a patient population for selected endpoints (e.g. plasma drug concentration, biomarkers, clinical outcomes). In most practical cases, such data are reported as aggregated summary statistics (mean values, medians, quantiles, survival curves, etc.). Each of these statistics can be conceptually linked to model outputs, defining population-level targets that the QSP model should reproduce at the individual level.\n\nExperimental data are decomposed into experimental metrics, where each metric represents a single aggregated statistic (e.g. mean concentration at a specific time point, a survival curve, or a quantile value). Each metric is encoded as one row in a tabular format. The table specifies the metric type, its parameters (statistic value, dispersion measures when available), and references to the corresponding model outputs to be used for comparison (e.g. a column representing plasma concentration at a given time point).\n\nDuring data loading, the metric table is processed using parse_metric_bindings. Each row is converted into a MetricBinding object, which combines a concrete AbstractMetric instance with references to the model output variables used for comparison. A MetricBinding therefore establishes an explicit link between an experimental metric and simulated individual-level data.\n\nThe resulting collection, Vector{MetricBinding}, represents the complete set of experimental targets and can be reused across different analyses. It serves as the primary input for loss computation between simulated individuals and experimental data.\n\nIn parallel, the user generates individual-level simulations of a QSP model for a virtual patient population. Simulation results are stored in a tabular format in CSV file which then can be loaded into a table-like structure DataFrame, where each row corresponds to one virtual individual and columns contain model outputs such as concentrations, biomarkers, or event times. Depending on the task (e.g. parameter calibration, virtual population selection, or model comparison), these simulation results can be combined with the metric bindings to evaluate model performance.\n\nA standard entry point is the get_loss function, which takes a Vector{MetricBinding} and a table of individual simulations and returns a scalar loss value representing the overall mismatch.\n\nFor finer-grained control, metric-level methods such as mismatch and mismatch_expression allow evaluation of individual metrics independently. Users may also implement custom analysis or optimization procedures that operate directly on MetricBinding objects and individual simulation data.","category":"section"},{"location":"key-concepts/#Typical-use-case","page":"Key Concepts","title":"Typical use case","text":"Model calibration to summary endpoints   Define experimental target statistics (means, quantiles, survival curves, category proportions) using a unified metric-based data format, and compute statistically grounded mismatch between individual simulations and reported population-level data.\nVirtual population selection and weighting   Evaluate large sets of simulated individuals against experimental metrics and use the resulting loss values to select or weight virtual patients so that the virtual population reproduces observed population variability.\nComparison of model variants or assumptions   Apply the same set of experimental metrics to different model versions, parameterizations, or simulation scenarios, and compare their ability to reproduce published summary statistics in a consistent way.\nReusable experimental target definitions   Store experimental metrics in a structured, tool-independent format that can be reused across multiple studies, models, or optimization methods.","category":"section"},{"location":"key-concepts/#Scope-and-non-goals","page":"Key Concepts","title":"Scope and non-goals","text":"DigiPopData.jl does not implement QSP models or simulation engines. It assumes that individual-level simulation results are generated externally.\nThe package does not provide optimization algorithms or virtual population selection methods. Instead, it is designed to be used as a building block within external calibration, optimization, or selection workflows.\nThe package does not attempt to reconstruct individual-level experimental data from aggregated summary statistics, as such reconstruction is fundamentally ill-posed and non-identifiable.","category":"section"},{"location":"metric-data-format/#Metric-Data-Format","page":"Metric Data Format","title":"Metric Data Format","text":"Experimental and clinical data are represented in DigiPopData.jl as a collection of metrics, i.e. aggregated summary statistics describing real patient populations. Each metric defines a population-level experimental target that can be compared to individual-level simulations.","category":"section"},{"location":"metric-data-format/#Internal-Julia-representation","page":"Metric Data Format","title":"Internal Julia representation","text":"Internally, each row of experimental data is represented as a MetricBinding. A MetricBinding links:\n\nan experimental metric (e.g. mean, quantile, category, survival),\na scenario (e.g. treatment arm),\nand an endpoint column in the simulation table.\n\nExample:\n\nmb = MetricBinding(\n    \"m_mean_conc24_Tx\",     # metric id\n    \"Tx\",                   # scenario (e.g. treatment arm)\n    MeanMetric(\n        40,                 # experimental sample size\n        2.1,                # mean value\n        0.2                 # standard deviation\n    ),\n    \"conc_t24\",             # endpoint column in simulation data\n    true                    # active flag\n)","category":"section"},{"location":"metric-data-format/#Tabular-metric-definition","page":"Metric Data Format","title":"Tabular metric definition","text":"For practical workflows, metrics are usually defined in a table and loaded in bulk (e.g. from CSV or DataFrame) using parse_metric_bindings. Each row corresponds to one metric.","category":"section"},{"location":"metric-data-format/#Core-columns","page":"Metric Data Format","title":"Core columns","text":"A metric table typically includes:\n\nid — unique object identifier  \nactive — whether the metric is included in the loss (1 or 0)  \nscenario — scenario identifier used to match simulation conditions  \nendpoint — name of the simulation output column used for comparison  \nmetric.type — metric type (e.g. mean, mean_sd, category, quantile, survival)  \nmetric.size — experimental sample size\nmetric.<prop> — additional metric-specific properties, see more details in Overview","category":"section"},{"location":"metric-data-format/#Example-table","page":"Metric Data Format","title":"Example table","text":"The table below defines two metrics for the same scenario Tx:\n\nid active scenario metric.type metric.size endpoint metric.mean metric.sd metric.levels metric.values\nm_conc24_mean_Tx 1 Tx mean 40 conc_t24 2.10 0.2  \nm_biomarker_q_Tx 1 Tx quantile 40 biomarker   0.25;0.50;0.75 0.1;1.35;10.1\n\nInterpretation:\n\nm_conc24_mean_Tx targets the mean of conc_t24 in the experimental population.\nm_biomarker_q_Tx targets the quantiles (0.25, 0.50, 0.75) of biomarker.\n\nIn practice, you may store only the columns required by the metric types used in your dataset.","category":"section"},{"location":"metric-data-format/#Loading-from-CSV","page":"Metric Data Format","title":"Loading from CSV","text":"using CSV, DataFrames\n\nmetrics_df = CSV.File(\"metrics.csv\") |> DataFrame\nmetrics = parse_metric_bindings(metrics_df)","category":"section"},{"location":"#README","page":"README","title":"README","text":"This documentation describes DigiPopData.jl, a Julia package for comparing individual-level QSP simulations with aggregated experimental data using metric-based representations and loss functions.","category":"section"},{"location":"#Content","page":"README","title":"Content","text":"Home\nREADME\nKey Concepts\nGetting Started\nSimulation Data Format\nMetric Data Format\nMetrics\nOverview\nMean\nMeanSD\nCategory\nQuantile\nSurvival\nReference\nAPI","category":"section"},{"location":"#External-resources","page":"README","title":"External resources","text":"Source code: https://github.com/hetalang/DigiPopData.jl  \nIssue tracker: https://github.com/hetalang/DigiPopData.jl/issues","category":"section"},{"location":"#License","page":"README","title":"License","text":"This project is licensed under the MIT License. See the LICENSE file for details.\n\nCopyright (c) 2025-2026 Heta project","category":"section"}]
}
